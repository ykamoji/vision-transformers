{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:13.337462Z",
     "start_time": "2024-03-23T00:15:11.167165Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models import create_model\n",
    "from vit_models import VisionTransformer\n",
    "import lightning as L\n",
    "from lightning import Trainer\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_n_params(module):\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def assert_tensors_equal(t1, t2):\n",
    "    a1, a2, = t1.detach().numpy(), t2.detach().numpy()\n",
    "    np.testing.assert_allclose(a1, a2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:14.345604Z",
     "start_time": "2024-03-23T00:15:14.341373Z"
    }
   },
   "id": "1dcbd662ccea4caf"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (patch_drop): Identity()\n  (norm_pre): Identity()\n  (blocks): Sequential(\n    (0): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (1): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (2): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (3): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (4): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (5): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (6): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (7): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (8): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (9): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (10): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n    (11): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (fc_norm): Identity()\n  (head_drop): Dropout(p=0.0, inplace=False)\n  (head): Linear(in_features=768, out_features=1000, bias=True)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"vit_base_patch16_384\"\n",
    "model_official = create_model(model_name, pretrained=True)\n",
    "model_official.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:17.514718Z",
     "start_time": "2024-03-23T00:15:16.098768Z"
    }
   },
   "id": "742a12c175a75888"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "custom_config = {\n",
    "        \"img_size\": 384,\n",
    "        \"in_chans\": 3,\n",
    "        \"patch_size\": 16,\n",
    "        \"embed_dim\": 768,\n",
    "        \"depth\": 12,\n",
    "        \"n_heads\": 12,\n",
    "        \"qkv_bias\": True,\n",
    "        \"mlp_ratio\": 4,\n",
    "        \"n_classes\": 1000\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:26.547265Z",
     "start_time": "2024-03-23T00:15:26.537447Z"
    }
   },
   "id": "bddec0a31d9ff7ee"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (blocks): ModuleList(\n    (0-11): 12 x Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): MLP(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (head): Linear(in_features=768, out_features=1000, bias=True)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_custom = VisionTransformer(**custom_config)\n",
    "model_custom.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:27.436091Z",
     "start_time": "2024-03-23T00:15:27.205551Z"
    }
   },
   "id": "173dc94c4049d81a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token | cls_token\n",
      "pos_embed | pos_embed\n",
      "patch_embed.proj.weight | patch_embed.proj.weight\n",
      "patch_embed.proj.bias | patch_embed.proj.bias\n",
      "blocks.0.norm1.weight | blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias | blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight | blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias | blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight | blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias | blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight | blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias | blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight | blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias | blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight | blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias | blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight | blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias | blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight | blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias | blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight | blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias | blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight | blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias | blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight | blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias | blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight | blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias | blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight | blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias | blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight | blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias | blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight | blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias | blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight | blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias | blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight | blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias | blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight | blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias | blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight | blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias | blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight | blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias | blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight | blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias | blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight | blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias | blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight | blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias | blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight | blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias | blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight | blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias | blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight | blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias | blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight | blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias | blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight | blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias | blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight | blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias | blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight | blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias | blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight | blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias | blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight | blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias | blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight | blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias | blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight | blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias | blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight | blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias | blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight | blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias | blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight | blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias | blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight | blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias | blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight | blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias | blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight | blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias | blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight | blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias | blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight | blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias | blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight | blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias | blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight | blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias | blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight | blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias | blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight | blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias | blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight | blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias | blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight | blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias | blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight | blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias | blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight | blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias | blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight | blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias | blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight | blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias | blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight | blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias | blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight | blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias | blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight | blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias | blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight | blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias | blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight | blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias | blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight | blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias | blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight | blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias | blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight | blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias | blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight | blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias | blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight | blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias | blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight | blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias | blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight | blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias | blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight | blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias | blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight | blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias | blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight | blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias | blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight | blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias | blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight | blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias | blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight | blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias | blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight | blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias | blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight | blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias | blocks.11.mlp.fc2.bias\n",
      "norm.weight | norm.weight\n",
      "norm.bias | norm.bias\n",
      "head.weight | head.weight\n",
      "head.bias | head.bias\n"
     ]
    }
   ],
   "source": [
    "for (n_o, p_o), (n_c, p_c) in zip(model_official.named_parameters(), model_custom.named_parameters()):\n",
    "    assert p_o.numel() == p_c.numel()\n",
    "    print(f\"{n_o} | {n_c}\")\n",
    "    p_c.data[:] = p_o.data\n",
    "    assert_tensors_equal(p_c.data, p_o.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:30.429542Z",
     "start_time": "2024-03-23T00:15:29.704657Z"
    }
   },
   "id": "a92b06b93ade3bf"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "for param in model_custom.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model_custom.head = nn.Linear(custom_config['embed_dim'], 200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:33.065545Z",
     "start_time": "2024-03-23T00:15:32.991632Z"
    }
   },
   "id": "54d112fa15459c9a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (blocks): ModuleList(\n    (0-11): 12 x Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): MLP(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (head): Linear(in_features=768, out_features=200, bias=True)\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_custom.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:35.857147Z",
     "start_time": "2024-03-23T00:15:35.841426Z"
    }
   },
   "id": "72935be5bbe45fdf"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "lr = 0.001\n",
    "compose = transforms.Compose([\n",
    "transforms.Resize(size=(384, 384), antialias=True),\n",
    "transforms.ToTensor()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:36.895503Z",
     "start_time": "2024-03-23T00:15:36.887936Z"
    }
   },
   "id": "5b7bb5edd47626db"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 n01443537 goldfish, Carassius auratus\n",
      "1 n01629819 European fire salamander, Salamandra salamandra\n",
      "2 n01641577 bullfrog, Rana catesbeiana\n",
      "3 n01644900 tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui\n",
      "4 n01698640 American alligator, Alligator mississipiensis\n",
      "5 n01742172 boa constrictor, Constrictor constrictor\n",
      "6 n01768244 trilobite\n",
      "7 n01770393 scorpion\n",
      "8 n01774384 black widow, Latrodectus mactans\n",
      "9 n01774750 tarantula\n",
      "10 n01784675 centipede\n",
      "11 n01855672 goose\n",
      "12 n01882714 koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus\n",
      "13 n01910747 jellyfish\n",
      "14 n01917289 brain coral\n",
      "15 n01944390 snail\n",
      "16 n01945685 slug\n",
      "17 n01950731 sea slug, nudibranch\n",
      "18 n01983481 American lobster, Northern lobster, Maine lobster, Homarus americanus\n",
      "19 n01984695 spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish\n",
      "20 n02002724 black stork, Ciconia nigra\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root='/Users/ykamoji/Documents/ImageDatabase/imageNet/tiny-imagenet-200/train', transform=compose)\n",
    "labels = {}\n",
    "for mapping in open('/Users/ykamoji/Documents/ImageDatabase/imageNet/tiny-imagenet-200/words.txt','r').readlines():\n",
    "    maps = mapping.replace('\\n','').split('\\t')\n",
    "    labels[maps[0]] = maps[1]\n",
    "\n",
    "def map_labels(dataset):\n",
    "    class_labels = {}\n",
    "    for ind in range(0, 100000, 500):\n",
    "        class_name = dataset.imgs[ind][0].split('/')[-1].split('_')[0]\n",
    "        class_label = dataset[ind][1]\n",
    "        class_labels[class_label] = class_name\n",
    "    return class_labels\n",
    "\n",
    "# for ind, (k,v) in enumerate(labels.items()):\n",
    "#     print(k,v)\n",
    "#     if ind > 10:\n",
    "#         break\n",
    "\n",
    "class_labels = map_labels(train_dataset)\n",
    "\n",
    "for ind, (k, v) in enumerate(class_labels.items()):\n",
    "    print(k, v, labels[v])\n",
    "    if ind == 20:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:40.062077Z",
     "start_time": "2024-03-23T00:15:39.618409Z"
    }
   },
   "id": "a069627ded073ee2"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class VisionTransformerWrapper(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(VisionTransformerWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=200)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        # print(outputs)\n",
    "        # print(labels)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.accuracy(outputs, labels)\n",
    "        \n",
    "        self.log('train_acc', self.accuracy)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "       train_dataset = torchvision.datasets.ImageFolder(root='/Users/ykamoji/Documents/ImageDatabase/imageNet/tiny-imagenet-200/train', transform=compose)\n",
    "       train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=11, persistent_workers=True)\n",
    "       return train_loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:15:42.486536Z",
     "start_time": "2024-03-23T00:15:42.455392Z"
    }
   },
   "id": "fc650cdc1ccc416d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | model    | VisionTransformer  | 86.2 M\n",
      "1 | accuracy | MulticlassAccuracy | 0     \n",
      "------------------------------------------------\n",
      "153 K     Trainable params\n",
      "86.1 M    Non-trainable params\n",
      "86.2 M    Total params\n",
      "344.977   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ddc59a450ad463cb86adea3430ac1db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "model_custom.train()\n",
    "trainer = Trainer(max_epochs = 10, fast_dev_run=True)\n",
    "model = VisionTransformerWrapper(model_custom)\n",
    "trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T00:17:22.996216Z",
     "start_time": "2024-03-23T00:15:45.895132Z"
    }
   },
   "id": "f0c86e09c3097c9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cd8f1b28ab1d7c52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
