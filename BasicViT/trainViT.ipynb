{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "from lightning import Trainer\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from PIL import Image\n",
    "from vit_models import VisionTransformer\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451200b0848687df",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compose = transforms.Compose([\n",
    "# transforms.Resize(size=(384, 384), antialias=True),\n",
    "transforms.ToTensor()\n",
    "])\n",
    "# plt.imshow(torch.permute(resize(train_dataset[index][0]), (1,2,0)).numpy())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81272c2f79c426a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1476cd7b566d2576",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root='/Users/ykamoji/Documents/ImageDatabase/imageNet/tiny-imagenet-200/train', transform=compose)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=11, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a36df4f6567daf",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for mapping in open('/Users/ykamoji/Documents/ImageDatabase/imageNet/tiny-imagenet-200/words.txt','r').readlines():\n",
    "    maps = mapping.replace('\\n','').split('\\t')\n",
    "    labels[maps[0]] = maps[1]\n",
    "\n",
    "def map_labels(dataset):\n",
    "    class_labels = {}\n",
    "    for ind in range(0, 100000, 500):\n",
    "        class_name = dataset.imgs[ind][0].split('/')[-1].split('_')[0]\n",
    "        class_label = dataset[ind][1]\n",
    "        class_labels[class_label] = class_name\n",
    "    return class_labels\n",
    "\n",
    "# for ind, (k,v) in enumerate(labels.items()):\n",
    "#     print(k,v)\n",
    "#     if ind > 10:\n",
    "#         break\n",
    "\n",
    "class_labels = map_labels(train_dataset)\n",
    "\n",
    "for ind, (k, v) in enumerate(class_labels.items()):\n",
    "    print(k, v, labels[v])\n",
    "    if ind == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac02b8f80c6bc38",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = 4600\n",
    "plt.imshow(torch.permute(train_dataset[index][0], (1,2,0)).numpy())\n",
    "label_class = train_dataset.imgs[index][0].split('/')[-1].split('_')[0]\n",
    "label_num = train_dataset[index][1]\n",
    "print(label_num, label_class, labels[class_labels[label_num]])\n",
    "# class_name = class_labels[labels[label_num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65e8fb0d6dfac7",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_config = {\n",
    "        \"img_size\": 64,\n",
    "        \"in_chans\": 3,\n",
    "        \"patch_size\": 16,\n",
    "        \"embed_dim\": 768,\n",
    "        \"depth\": 12,\n",
    "        \"n_heads\": 12,\n",
    "        \"qkv_bias\": True,\n",
    "        \"mlp_ratio\": 4,\n",
    "        \"n_classes\":200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f3fcdeabb3819",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformerWrapper(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, model, **kvargs):\n",
    "        super(VisionTransformerWrapper, self).__init__()\n",
    "        self.model = model(**kvargs)\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=200)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        # print(images.shape)\n",
    "        outputs = self(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        self.accuracy(outputs, labels)\n",
    "        \n",
    "        self.log('train_acc_step', self.accuracy)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "       train_dataset = torchvision.datasets.ImageFolder(root='/Users/ykamoji/Documents/ImageDatabase/imageNet/tiny-imagenet-200/train', transform=compose)\n",
    "       train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=11, persistent_workers=True)\n",
    "       return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c75f54481f1b3",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(max_epochs = 5,fast_dev_run=True, accelerator=\"mps\", devices=1)\n",
    "model = VisionTransformerWrapper(VisionTransformer, **custom_config)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94594a3c-ae2e-48ba-b505-1101d678b414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a8611-6d29-4414-93d1-48ef1edb3678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = train_dataset[0][0]\n",
    "# img = (np.array(img) / 128) - 1  # in the range -1, 1\n",
    "print(img.shape)\n",
    "logits = model(img.unsqueeze(0).to(torch.float32))\n",
    "# print(logits)\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "top_probs, top_ixs = probs[0].topk(5)\n",
    "# print(f\"\\n{image}\")\n",
    "for i, (ix_, prob_) in enumerate(zip(top_ixs, top_probs)):\n",
    "    ix = ix_.item()\n",
    "    prob = prob_.item()\n",
    "    cls = labels[class_labels[ix]]\n",
    "    print(f\"{i}: {cls:<45} --- {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67ffc7-44b0-4bc4-8487-79dfd101bbe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2ca9a-0d75-433c-a2de-676ad3afca56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
